{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3703bea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Nearest Neighborhood\n",
    "\n",
    "KNN was born out of research done for the armed forces. Fix and Hodge - two officers of USAF School of Aviation Medicine - wrote a technical report in 1951 introducing the KNN algorithm. \n",
    "\n",
    "KNN can be used in both <font color=red>*__Regression__*</font> and <font color=red>*__Classification__*</font> predictive problems. However, it’s mostly used in classification since it fairs across all parameters evaluated when determining the usability of a technique\n",
    "<font color=red>*__Prediction Power__*</font>\n",
    "<font color=red>*__Calculation Time__*</font>\n",
    "\n",
    "__It is used due to its ease of interpretation and low calculation time.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb4d14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Companies like <font color=red>*__Amazon__*</font> or <font color=red>*__Netflix__*</font> use KNN when recommending books to buy or movies to watch. \n",
    "\n",
    "How do these companies make recommendations? \n",
    "\n",
    "Well, these companies gather data on the books you have read or movies you have watched on their website and apply KNN. The companies will input your available customer data and compare that to other customers who have purchased similar books or have watched similar movies. \n",
    "\n",
    "The books and movies recommended depending on how the algorithm classifies that data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8a708",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does KNN works?\n",
    "\n",
    "The k-nearest neighbor algorithm stores all the available data and classifies a new data point based on the similarity measure (e.g., distance functions). This means when new data appears. Then it can be easily classified into a well-suited category by using K-NN algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3109e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"knn_demo.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" style=\"float:right\" width=350px;/>\n",
    "\n",
    "Suppose there are two classes, \n",
    "\n",
    "i.e., <font color=red>Class A</font> and <font color=green>Class B</font>, \n",
    "\n",
    "and we have a new unknown data point “?”, \n",
    "\n",
    "so this data point will lie in which of these classes. To solve this problem, we need a K-NN algorithm. The data point is classified by a majority vote of its neighbors, with the data point being assigned to the class most common amongst its K nearest neighbors measured by a distance function.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a56c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we can see that if k = 3, then based on the distance function used, the nearest three neighbors of the data point is found and based on the majority votes of its neighbors, the data point is classified into a class. \n",
    "\n",
    "In the case of k = 3, for the above diagram, it's Class B. \n",
    "\n",
    "Similarly, when k = 7, for the above diagram, based on the majority votes of its neighbors, the data point is classified to Class A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1eb8ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbors\n",
    "KNN algorithm applies the birds of a feather. It assumes that similar things are near to each other; that is, they are nearby. \n",
    "\n",
    "The idea of similarity (sometimes called closeness, proximity, or distance).\n",
    "\n",
    "Euclidean distance or straight-line distance is a popular and familiar choice of calculating distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c8e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the right value for K\n",
    "To get the right K, you should run the KNN algorithm several times with different values of K and select the one that has the least number of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af188775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-As K approaches 1, your prediction becomes less stable. \n",
    "\n",
    "-As your value of K increases, your prediction becomes more stable due to the majority of voters.\n",
    "\n",
    "-When you start receiving an increasing number of errors, you should know you are pushing your K too far. \n",
    "\n",
    "\n",
    "-Taking a majority vote among labels needs K to be an odd number to have a tiebreaker. - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27021b06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working of KNN Algorithm in Machine\n",
    "\n",
    "Step 1 – When implementing an algorithm, you will always need a data set. So, you start by loading the training and the test data.\n",
    "\n",
    "Step 2 – Choose the nearest data points (the value of K). K can be any integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1690d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 3:\n",
    "\n",
    "3.1 – Use Euclidean distance, Hamming, or Manhattan to calculate the distance between test data and each row of training. The Euclidean method is the most used when calculating distance. \n",
    "\n",
    "3.2 – Sort data set in ascending order based on the distance value. \n",
    "\n",
    "3.3 – From the sorted array, choose the top K rows.\n",
    "\n",
    "3.4 – Based on the most appearing class of these rows, it will assign a class to the test point.\n",
    "\n",
    "Step 4 – End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603901b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of KNN\n",
    "1. Quick calculation time\n",
    "2. Simple algorithm – to interpret \n",
    "3. Versatile – useful for regression and classification\n",
    "4. High accuracy – you do not need to compare with better-supervised learning models\n",
    "5. No assumptions about data – no need to make additional assumptions, tune several parameters, or build a model. This makes it crucial in nonlinear data case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767c114",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages of KNN\n",
    "1. Accuracy depends on the quality of the data\n",
    "2. With large data, the prediction stage might be slow\n",
    "3. Sensitive to the scale of the data and irrelevant features\n",
    "4. Require high memory – need to store all of the training data\n",
    "5. Given that it stores all of the training, it can be computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdbff5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3e7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280222b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd50ec41",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris=pd.read_csv(\"iris.csv\")\n",
    "iris=iris.iloc[:,1:]\n",
    "#iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb8a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fa1e5a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#iris.groupby(\"Species\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85228b50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Divide Data into features and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "075145dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Iris-setosa\n",
       "1         Iris-setosa\n",
       "2         Iris-setosa\n",
       "3         Iris-setosa\n",
       "4         Iris-setosa\n",
       "            ...      \n",
       "145    Iris-virginica\n",
       "146    Iris-virginica\n",
       "147    Iris-virginica\n",
       "148    Iris-virginica\n",
       "149    Iris-virginica\n",
       "Name: Species, Length: 150, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()\n",
    "X=iris.iloc[:,0:4] # features\n",
    "Y=iris.iloc[:,4] # Labels\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd88d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf6ba736",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE=LabelEncoder()\n",
    "Y=LE.fit_transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b5b59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa069aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731d528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create and train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7ed7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier=KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592b16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Make predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6f850c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2 2 0 2 1 2 1 0 1 2 0 2 1 2 2 0 2 2 1 0 1 2 2 0 0 1 2 2 2 1 0 1 2 1 0\n",
      " 1 2 2 0 2 1 1 2 2 1 1 1 1]\n",
      "[0 1 1 2 0 2 1 1 1 0 1 1 0 2 1 2 2 0 2 2 1 0 1 2 2 0 0 1 2 2 2 1 0 1 2 1 0\n",
      " 1 2 2 0 2 1 1 2 2 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_test)\n",
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88588d40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4150eb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is  92.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm\n",
    "\n",
    "print(\"Accuracy of the model is \",accuracy_score(y_test,y_pred)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7680f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate alternative K-values for better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "150cc59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list=list(range(1,50,2))\n",
    "acc_score=[]\n",
    "err_rate=[]\n",
    "for x in k_list:\n",
    "    classifier=KNeighborsClassifier(n_neighbors=x)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred=classifier.predict(x_test)\n",
    "    acc_score.append(accuracy_score(y_test,y_pred))\n",
    "    err_rate.append(1-accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc63016",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b328bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"best k:\",k_list[err_rate.index(min(err_rate))])\n",
    "\n",
    "acc_score[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1015e0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adjust K value per error rate evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ae0dbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is  0.98 %\n",
      "Error rate of the model is  0.02 %\n"
     ]
    }
   ],
   "source": [
    "classifier=KNeighborsClassifier(n_neighbors=9)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "print(\"Accuracy of the model is \",accuracy_score(y_test,y_pred),\"%\")\n",
    "print(\"Error rate of the model is \",round(1-accuracy_score(y_test,y_pred),2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68bffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7b117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
